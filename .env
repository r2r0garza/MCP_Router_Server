# MCP Router Server Environment Example

# Core LLM provider selection
LLM_PROVIDER=azure #openai, lmstudio, openrouter, ollama, anthropic, azure
MODEL=gpt-4o-mini

# LM Studio (local inference endpoint)
LMSTUDIO_URL=http://localhost:1234/v1/chat/completions
LMSTUDIO_API_KEY=hermes-3-llama-3.1-8b

ANTHROPIC_API_KEY=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

OPENROUTER_API_KEY=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

AZURE_FOUNDY_API_KEY=EhxGoubb5tnXM9n3omPCWM7ZZEG0HgwOK5wuMqvyzxJJHZhqLruNJQQJ99ALACYeBjFXJ3w3AAABACOGOKjW
AZURE_FOUNDY_DEPLOYMENT=ai-quality-assistance #i.e - in https://my-deployment.openai.azure.com, the deployement is my-deployment
VERSION=2025-01-01-preview #i.e - 2023-03-15-preview